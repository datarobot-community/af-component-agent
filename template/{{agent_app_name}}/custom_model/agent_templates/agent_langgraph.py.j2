# Copyright 2025 DataRobot, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from datetime import datetime
from typing import Any, AsyncGenerator, Optional, Union

from config import Config
from datarobot_genai.agents import (
    BaseAgent,
    extract_user_prompt_content,
    make_system_prompt,
)
from datarobot_genai.agents.langgraph import create_pipeline_interactions_from_events
from langchain_core.messages import HumanMessage
from langchain_litellm.chat_models import ChatLiteLLM
from langgraph.graph import END, START, MessagesState, StateGraph
from langgraph.prebuilt import create_react_agent
from langgraph.types import Command
from openai.types.chat import CompletionCreateParams


class MyAgent(BaseAgent):
    """MyAgent is a custom agent that uses Langgraph to plan, write, and edit content.
    It utilizes DataRobot's LLM Gateway or a specific deployment for language model interactions.
    This example illustrates 3 agents that handle content creation tasks, including planning, writing,
    and editing blog posts.
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        api_base: Optional[str] = None,
        model: Optional[str] = None,
        verbose: Optional[Union[bool, str]] = True,
        timeout: Optional[int] = 90,
        **kwargs: Any,
    ):
        """Initializes the MyAgent class with API key, base URL, model, and verbosity settings.

        Args:
            api_key: Optional[str]: API key for authentication with DataRobot services.
                Defaults to None, in which case it will use the DATAROBOT_API_TOKEN environment variable.
            api_base: Optional[str]: Base URL for the DataRobot API.
                Defaults to None, in which case it will use the DATAROBOT_ENDPOINT environment variable.
            model: Optional[str]: The LLM model to use.
                Defaults to None.
            verbose: Optional[Union[bool, str]]: Whether to enable verbose logging.
                Accepts boolean or string values ("true"/"false"). Defaults to True.
            timeout: Optional[int]: How long to wait for the agent to respond.
                Defaults to 90 seconds.
            **kwargs: Any: Additional keyword arguments passed to the agent.
                Contains any parameters received in the CompletionCreateParams.

        Returns:
            None
        """
        super().__init__(
            api_key=api_key,
            api_base=api_base,
            model=model,
            verbose=verbose,
            timeout=timeout,
            **kwargs,
        )
        self.config = Config()
        self.default_model = self.config.{{ _external_data.llm.llm_app_name|replace("-", "_") }}_default_model
        self.model = model
        self.timeout = timeout
        if isinstance(verbose, str):
            self.verbose = verbose.lower() == "true"
        elif isinstance(verbose, bool):
            self.verbose = verbose

    async def invoke(
        self, completion_create_params: CompletionCreateParams
    ) -> Union[
        AsyncGenerator[tuple[str, Any | None, dict[str, int]], None],
        tuple[str, Any | None, dict[str, int]],
    ]:
        """Run the agent with the provided completion parameters.

        [THIS METHOD IS REQUIRED FOR THE AGENT TO WORK WITH DRUM SERVER]

        Args:
            completion_create_params: The completion request parameters including input topic and settings.
        Returns:
            Union[
                Generator[tuple[str, Any | None, dict[str, int]], None, None],
                tuple[str, Any | None, dict[str, int]],
            ]: For streaming requests, returns a generator yielding tuples of (response_text, pipeline_interactions, usage_metrics).
               For non-streaming requests, returns a single tuple of (response_text, pipeline_interactions, usage_metrics).
        """
        # Retrieve the starting user prompt from the CompletionCreateParams
        user_prompt_content = extract_user_prompt_content(completion_create_params)

        # Print commands may need flush=True to ensure they are displayed in real-time.
        print("Running agent with user prompt:", user_prompt_content, flush=True)

        # Construct the input message for the langgraph graph.
        input_message = Command(
            update={
                "messages": (
                    "user",
                    f"The topic is '{user_prompt_content}'. Make sure you find any interesting and relevant "
                    f"information given the current year is {str(datetime.now().year)}.",
                ),
            },
            goto="writer_node",
        )

        # Create and invoke the Langgraph Agentic Workflow with the inputs
        langgraph_workflow = StateGraph(MessagesState)
        langgraph_workflow.add_node("planner_node", self.task_plan)
        langgraph_workflow.add_node("writer_node", self.task_write)
        langgraph_workflow.add_node("editor_node", self.task_edit)
        langgraph_workflow.add_edge(START, "planner_node")
        langgraph_execution_graph = langgraph_workflow.compile()

        graph_stream = langgraph_execution_graph.astream(
            input=input_message,
            config={
                "recursion_limit": 150
            },  # Maximum number of steps to take in the graph
            debug=True,
        )

        usage_metrics: dict[str, int] = {
            "completion_tokens": 0,
            "prompt_tokens": 0,
            "total_tokens": 0,
        }

        # The following code demonstrate both a synchronous and streaming response.
        # You can choose one or the other based on your use case, they function the same.
        # The main difference is returning a generator for streaming or a final response for sync.
        if completion_create_params.get("stream"):
            # Streaming response: yield each message as it is generated
            async def stream_generator() -> AsyncGenerator[
                tuple[str, Any | None, dict[str, int]], None
            ]:
                # For each event in the graph stream, yield the latest message content
                # along with updated usage metrics.
                events = []
                async for event in graph_stream:
                    events.append(event)
                    current_node = next(iter(event))
                    yield (
                        str(event[current_node]["messages"][-1].content),
                        None,
                        usage_metrics,
                    )
                    current_usage = event[current_node].get("usage", {})
                    if current_usage:
                        usage_metrics["total_tokens"] += current_usage.get(
                            "total_tokens", 0
                        )
                        usage_metrics["prompt_tokens"] += current_usage.get(
                            "prompt_tokens", 0
                        )
                        usage_metrics["completion_tokens"] += current_usage.get(
                            "completion_tokens", 0
                        )

                # Create a list of events from the event listener
                pipeline_interactions = create_pipeline_interactions_from_events(events)

                # yield the final response indicating completion
                yield "", pipeline_interactions, usage_metrics

            return stream_generator()
        else:
            # Synchronous response: collect all events and return the final message
            events = [event async for event in graph_stream]
            pipeline_interactions = create_pipeline_interactions_from_events(events)

            # Extract the final event from the graph stream as the synchronous response
            last_event = events[-1]
            node_name = next(iter(last_event))
            response_text = str(last_event[node_name]["messages"][-1].content)
            current_usage = last_event[node_name].get("usage", {})
            if current_usage:
                usage_metrics["total_tokens"] += current_usage.get("total_tokens", 0)
                usage_metrics["prompt_tokens"] += current_usage.get("prompt_tokens", 0)
                usage_metrics["completion_tokens"] += current_usage.get(
                    "completion_tokens", 0
                )

            return response_text, pipeline_interactions, usage_metrics

    def llm(
        self,
        preferred_model: str | None = None,
        auto_model_override: bool = True,
    ) -> ChatLiteLLM:
        """Returns the ChatLiteLLM to use for a given model.

        If a `preferred_model` is provided, it will be used. Otherwise, the default model will be used.
        If auto_model_override is True, it will try and use the model specified in the request
        but automatically back out to the default model if the LLM Gateway is not configured

        Args:
            preferred_model: Optional[str]: The model to use. If none, it defaults to config.llm_default_model.
            auto_model_override: Optional[bool]: If True, it will try and use the model
                specified in the request but automatically back out if the LLM Gateway is
                not available.

        Returns:
            ChatLiteLLM: The model to use.
        """
        api_base = self.litellm_api_base(self.config.{{ _external_data.llm.llm_app_name|replace("-", "_") }}_deployment_id)
        model = preferred_model
        if preferred_model is None:
            model = self.default_model
        if auto_model_override and not self.config.use_datarobot_llm_gateway:
            model = self.default_model
        if self.verbose:
            print(f"Using model: {model}")
        return ChatLiteLLM(
            model=model,
            api_base=api_base,
            api_key=self.api_key,
            timeout=self.timeout,
        )

    @property
    def agent_planner(self) -> Any:
        return create_react_agent(
            self.llm(preferred_model="datarobot/azure/gpt-4o-mini"),
            tools=[],
            prompt=make_system_prompt(
                "You are a content planner. You are working with a content writer and editor colleague.\n"
                "You're working on planning a blog article about the topic. You collect information that helps the "
                "audience learn something and make informed decisions. Your work is the basis for the Content Writer "
                "to write an article on this topic."
                "\n"
                "1. Prioritize the latest trends, key players, and noteworthy news on the topic.\n"
                "2. Identify the target audience, considering their interests and pain points.\n"
                "3. Develop a detailed content outline including an introduction, key points, and a call to action.\n"
                "4. Include SEO keywords and relevant data or sources."
                "\n"
                "Plan engaging and factually accurate content on the topic. You must create a comprehensive content "
                "plan document with an outline, audience analysis, SEO keywords, and resources.",
            ),
        )

    @property
    def agent_writer(self) -> Any:
        return create_react_agent(
            self.llm(preferred_model="datarobot/azure/gpt-4o-mini"),
            tools=[],
            prompt=make_system_prompt(
                "You are a content writer. You are working with a planner and editor colleague.\n"
                "You're working on writing a new opinion piece about the topic. You base your writing on the work "
                "of the Content Planner, who provides an outline and relevant context about the topic. You follow "
                "the main objectives and direction of the outline, as provided by the Content Planner. You also "
                "provide objective and impartial insights and back them up with information provided by the Content "
                "Planner. You acknowledge in your opinion piece when your statements are opinions as opposed to "
                "objective statements.\n"
                "1. Use the content plan to craft a compelling blog post.\n"
                "2. Incorporate SEO keywords naturally.\n"
                "3. Sections/Subtitles are properly named in an engaging manner.\n"
                "4. Ensure the post is structured with an engaging introduction, insightful body, and a summarizing "
                "conclusion.\n"
                "5. Proofread for grammatical errors and alignment with the brand's voice.\n"
                "Write insightful and factually accurate opinion piece about the topic. You must create a "
                "well-written blog post in markdown format, ready for publication, each section should have 2 or 3 "
                "paragraphs.",
            ),
        )

    @property
    def agent_editor(self) -> Any:
        return create_react_agent(
            self.llm(),
            tools=[],
            prompt=make_system_prompt(
                "You are a content editor. You are working with a planner and writer colleague.\n"
                "You are an editor who receives a blog post from the Content Writer. Your goal is to review the "
                "blog post to ensure that it follows journalistic best practices, provides balanced viewpoints when "
                "providing opinions or assertions, and also avoids major controversial topics or opinions when "
                "possible.\n"
                "Proofread the given blog post for grammatical errors and alignment with the brand's voice.\n"
                "You must create a well-written blog post in markdown format, ready for publication, each section "
                "should have 2 or 3 paragraphs.\n"
                "You should return ONLY the full corrected blog post in markdown format, "
                "do not include any commentary or explanations.",
            ),
        )

    def task_plan(self, state: MessagesState) -> Command[Any]:
        result = self.agent_planner.invoke(state)
        result["messages"][-1] = HumanMessage(
            content=result["messages"][-1].content, name="planner_node"
        )
        return Command(
            update={
                # share internal message history with other agents
                "messages": result["messages"],
            },
            goto="writer_node",
        )

    def task_write(self, state: MessagesState) -> Command[Any]:
        result = self.agent_writer.invoke(state)
        result["messages"][-1] = HumanMessage(
            content=result["messages"][-1].content, name="writer_node"
        )
        return Command(
            update={
                # share internal message history with other agents
                "messages": result["messages"],
            },
            goto="editor_node",
        )

    def task_edit(self, state: MessagesState) -> Command[Any]:
        result = self.agent_editor.invoke(state)
        result["messages"][-1] = HumanMessage(
            content=result["messages"][-1].content, name="editor_node"
        )
        return Command(
            update={
                # share internal message history with other agents
                "messages": result["messages"],
            },
            goto=END,
        )
