# Copyright 2025 DataRobot, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import json
import time
import uuid
{%- if agent_template_framework == "langgraph" %}
from typing import Any, Dict, List, Union
{%- else %}
from typing import Any, Dict, Union
{%- endif %}
{% if agent_template_framework == "crewai" %}
from crewai import CrewOutput
{%- endif %}
from openai.types import CompletionUsage
from openai.types.chat import (
    ChatCompletion,
    ChatCompletionMessage,
    CompletionCreateParams,
)
from openai.types.chat.chat_completion import Choice
from ragas import MultiTurnSample
{%- if agent_template_framework == "langgraph" %}
from ragas.integrations.langgraph import convert_to_ragas_messages
{%- endif %}

class CustomModelChatResponse(ChatCompletion):
    pipeline_interactions: str | None = None


def create_inputs_from_completion_params(
    completion_create_params: CompletionCreateParams,
) -> Union[Dict[str, Any], str]:
    """Load the user prompt from a JSON string or file."""
    input_prompt: Any = next(
        (
            msg
            for msg in completion_create_params["messages"]
            if msg.get("role") == "user"
        ),
        {},
    )
    if len(input_prompt) == 0:
        raise ValueError("No user prompt found in the messages.")
    user_prompt = input_prompt.get("content")

    try:
        inputs = json.loads(user_prompt)
    except json.JSONDecodeError:
        inputs = user_prompt

    return inputs  # type: ignore[no-any-return]


def create_completion_from_response_text(
    response_text: str,
    usage_metrics: Dict[str, int],
    pipeline_interactions: MultiTurnSample | None = None,
) -> CustomModelChatResponse:
    """Convert the text of the LLM response into a chat completion response."""
    completion_id = str(uuid.uuid4())
    completion_timestamp = int(time.time())

    choice = Choice(
        index=0,
        message=ChatCompletionMessage(role="assistant", content=response_text),
        finish_reason="stop",
    )
    completion = CustomModelChatResponse(
        id=completion_id,
        object="chat.completion",
        choices=[choice],
        created=completion_timestamp,
        model="MODEL_NAME",
        usage=CompletionUsage(**usage_metrics),
        pipeline_interactions=pipeline_interactions.model_dump_json()
        if pipeline_interactions
        else None,
    )
    return completion

{% if agent_template_framework == "crewai" %}
def to_custom_model_response(
    crew_output: CrewOutput,
) -> CustomModelChatResponse:
    usage_metrics: Dict[str, int] = {
        "completion_tokens": crew_output.token_usage.completion_tokens,
        "prompt_tokens": crew_output.token_usage.prompt_tokens,
        "total_tokens": crew_output.token_usage.total_tokens,
    }

    response = create_completion_from_response_text(
        response_text=str(crew_output.raw),
        usage_metrics=usage_metrics,
        pipeline_interactions=None,
    )
    return response
{% elif agent_template_framework == "langgraph" %}
def _extract_pipeline_interactions(events):
    messages = []
    for e in events:
        for k, v in e.items():
            messages.extend(v["messages"])
    ragas_trace = convert_to_ragas_messages(messages)
    pipeline_interactions = MultiTurnSample(user_input=ragas_trace)
    return pipeline_interactions


def to_custom_model_response(
    events: List[dict[str, Any]], usage_metrics: Dict[str, int]
) -> CustomModelChatResponse:
    response = events[-1]
    node_name = next(iter(response))
    output = str(response[node_name]["messages"][-1].content)

    pipeline_interactions = _extract_pipeline_interactions(events)

    response = create_completion_from_response_text(
        response_text=output,
        usage_metrics=usage_metrics,
        pipeline_interactions=pipeline_interactions,
    )
    return response
{% elif agent_template_framework == "llamaindex" %}
def to_custom_model_response(
        agent_result: str,
        usage_metrics: Dict[str, int],
) -> CustomModelChatResponse:
    response = create_completion_from_response_text(
        response_text=agent_result,
        usage_metrics=usage_metrics,
        pipeline_interactions=None,
    )
    return response
{%- endif %}
