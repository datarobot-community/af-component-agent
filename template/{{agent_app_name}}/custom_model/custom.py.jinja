# Copyright 2025 DataRobot, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ------------------------------------------------------------------------------
# THIS SECTION OF CODE IS REQUIRED TO SETUP TRACING AND TELEMETRY FOR THE AGENTS.
# REMOVING THIS CODE WILL DISABLE ALL MONITORING, TRACING AND TELEMETRY.
# isort: off
from datarobot_genai.core.telemetry_agent import instrument

{% if agent_template_framework == "crewai" -%}
instrument(framework="crewai")
{% elif agent_template_framework == "langgraph" -%}
instrument(framework="langgraph")
{% elif agent_template_framework == "llamaindex" -%}
instrument(framework="llamaindex")
{% elif agent_template_framework == "nat" -%}
instrument(framework="nat")
{% else -%}
instrument()
{% endif -%}

# ruff: noqa: E402
from agent import MyAgent
from config import Config

# isort: on
# ------------------------------------------------------------------------------
import asyncio
import os
from concurrent.futures import ThreadPoolExecutor
from typing import Any, AsyncGenerator, Iterator, Union

from datarobot.models.genai.agent.auth import (
    get_authorization_context,
    set_authorization_context,
)
from datarobot_genai.core.chat import (
    CustomModelChatResponse,
    CustomModelStreamingResponse,
    initialize_authorization_context,
    to_custom_model_chat_response,
    to_custom_model_streaming_response,
)
from openai.types.chat import CompletionCreateParams
from openai.types.chat.completion_create_params import (
    CompletionCreateParamsNonStreaming,
    CompletionCreateParamsStreaming,
)


def load_model(code_dir: str) -> tuple[ThreadPoolExecutor, asyncio.AbstractEventLoop]:
    """The agent is instantiated in this function and returned."""
    thread_pool_executor = ThreadPoolExecutor(1)
    event_loop = asyncio.new_event_loop()
    thread_pool_executor.submit(asyncio.set_event_loop, event_loop).result()
    return (thread_pool_executor, event_loop)


{% if agent_template_framework == "nat" -%}
import queue
import time
import uuid
from typing import TypeVar
from collections.abc import AsyncIterator
from ragas import MultiTurnSample
from openai.types.chat.chat_completion_chunk import Choice as ChunkChoice
from openai.types.chat.chat_completion_chunk import ChoiceDelta
from openai.types import CompletionUsage

T = TypeVar("T")

def async_gen_to_sync_thread(
    async_iterator: AsyncIterator[T],
    thread_pool_executor: ThreadPoolExecutor,
    event_loop: asyncio.AbstractEventLoop,
) -> Iterator[T]:
    """Run an async iterator in a separate thread and provide a sync iterator."""
    # A thread-safe queue for communication
    sync_queue: queue.Queue[Any] = queue.Queue()
    # A sentinel object to signal the end of the async generator
    SENTINEL = object()  # noqa: N806

    async def run_async_to_queue() -> None:
        """Run in the separate thread's event loop."""
        try:
            async for item in async_iterator:
                sync_queue.put(item)
        except Exception as e:
            # Put the exception on the queue to be re-raised in the main thread
            sync_queue.put(e)
        finally:
            # Signal the end of iteration
            sync_queue.put(SENTINEL)

    thread_pool_executor.submit(event_loop.run_until_complete, run_async_to_queue()).result()

    # The main thread consumes items synchronously
    while True:
        item = sync_queue.get()
        if item is SENTINEL:
            break
        if isinstance(item, Exception):
            raise item
        yield item


def streaming_iterator_to_custom_model_streaming_response(
    streaming_response_iterator: Iterator[tuple[str, MultiTurnSample | None, dict[str, int]]],
    model: str | object | None,
) -> Iterator[CustomModelStreamingResponse]:
    """Convert the OpenAI ChatCompletionChunk response to CustomModelStreamingResponse."""
    completion_id = str(uuid.uuid4())
    created = int(time.time())

    last_pipeline_interactions = None
    last_usage_metrics = None

    while True:
        try:
            (
                response_text,
                pipeline_interactions,
                usage_metrics,
            ) = next(streaming_response_iterator)
            last_pipeline_interactions = pipeline_interactions
            last_usage_metrics = usage_metrics

            if response_text:
                choice = ChunkChoice(
                    index=0,
                    delta=ChoiceDelta(role="assistant", content=response_text),
                    finish_reason=None,
                )
                yield CustomModelStreamingResponse(
                    id=completion_id,
                    object="chat.completion.chunk",
                    created=created,
                    model=model,
                    choices=[choice],
                    usage=CompletionUsage(**usage_metrics) if usage_metrics else None,
                )
        except StopIteration:
            break
    # Yield final chunk indicating end of stream
    choice = ChunkChoice(
        index=0,
        delta=ChoiceDelta(role="assistant"),
        finish_reason="stop",
    )
    yield CustomModelStreamingResponse(
        id=completion_id,
        object="chat.completion.chunk",
        created=created,
        model=model,
        choices=[choice],
        usage=CompletionUsage(**last_usage_metrics) if last_usage_metrics else None,
        pipeline_interactions=last_pipeline_interactions.model_dump_json()
        if last_pipeline_interactions
        else None,
    )


def chat(
    completion_create_params: CompletionCreateParams
    | CompletionCreateParamsNonStreaming
    | CompletionCreateParamsStreaming,
    load_model_result: tuple[ThreadPoolExecutor, asyncio.AbstractEventLoop],
    **kwargs: Any,
) -> Union[CustomModelChatResponse, Iterator[CustomModelStreamingResponse]]:
    """When using the chat endpoint, this function is called.

    Agent inputs are in OpenAI message format and defined as the 'user' portion
    of the input prompt.

    Example:
        prompt = {
            "topic": "Artificial Intelligence",
        }
        client = OpenAI(...)
        completion = client.chat.completions.create(
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": f"{json.dumps(prompt)}"},
            ],
            extra_body = {
                "environment_var": True,
            },
            ...
        )
    """
    thread_pool_executor, event_loop = load_model_result

    # Change working directory to the directory containing this file.
    # Some agent frameworks expect this for expected pathing.
    os.chdir(os.path.dirname(os.path.abspath(__file__)))

    # Load MCP runtime parameters and session secret if configured
    # ["EXTERNAL_MCP_URL", "MCP_DEPLOYMENT_ID", "SESSION_SECRET_KEY"]
    _ = Config()

    # Initialize the authorization context for downstream agents and tools to retrieve
    # access tokens for external services.
    initialize_authorization_context(completion_create_params, **kwargs)

    # Get the authorization context from the main thread to propagate to the worker thread
    # ContextVars are thread-local, so we need to set it in the worker thread
    try:
        auth_context = get_authorization_context()
    except LookupError:
        auth_context = {}

    # Instantiate the agent, all fields from the completion_create_params are passed to the agent
    # allowing environment variables to be passed during execution
    agent = MyAgent(**completion_create_params)

    # Invoke the agent and check if it returns a generator or a tuple
    # Set the authorization context in the worker thread before invoking the agent
    def invoke_with_auth_context():  # type: ignore[no-untyped-def]
        try:
            set_authorization_context(auth_context)
        except AttributeError:
            pass

        return event_loop.run_until_complete(
            agent.invoke(completion_create_params=completion_create_params)
        )

    result = thread_pool_executor.submit(invoke_with_auth_context).result()

    # Check if the result is a generator (streaming response)
    if isinstance(result, AsyncGenerator):
        # Streaming response
        streaming_response_iterator = async_gen_to_sync_thread(result, thread_pool_executor, event_loop)
        return streaming_iterator_to_custom_model_streaming_response(
            streaming_response_iterator,
            model=completion_create_params.get("model"),
        )
    else:
        # Non-streaming response
        response_text, pipeline_interactions, usage_metrics = result

        return to_custom_model_chat_response(
            response_text,
            pipeline_interactions,
            usage_metrics,
            model=completion_create_params.get("model"),
        )
{% else -%}
def chat(
    completion_create_params: CompletionCreateParams
    | CompletionCreateParamsNonStreaming
    | CompletionCreateParamsStreaming,
    load_model_result: tuple[ThreadPoolExecutor, asyncio.AbstractEventLoop],
    **kwargs: Any,
) -> Union[CustomModelChatResponse, Iterator[CustomModelStreamingResponse]]:
    """When using the chat endpoint, this function is called.

    Agent inputs are in OpenAI message format and defined as the 'user' portion
    of the input prompt.

    Example:
        prompt = {
            "topic": "Artificial Intelligence",
        }
        client = OpenAI(...)
        completion = client.chat.completions.create(
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": f"{json.dumps(prompt)}"},
            ],
            extra_body = {
                "environment_var": True,
            },
            ...
        )
    """
    thread_pool_executor, event_loop = load_model_result

    # Change working directory to the directory containing this file.
    # Some agent frameworks expect this for expected pathing.
    os.chdir(os.path.dirname(os.path.abspath(__file__)))

    # Load MCP runtime parameters and session secret if configured
    # ["EXTERNAL_MCP_URL", "MCP_DEPLOYMENT_ID", "SESSION_SECRET_KEY"]
    _ = Config()

    # Initialize the authorization context for downstream agents and tools to retrieve
    # access tokens for external services.
    initialize_authorization_context(completion_create_params, **kwargs)

    # Get the authorization context from the main thread to propagate to the worker thread
    # ContextVars are thread-local, so we need to set it in the worker thread
    try:
        auth_context = get_authorization_context()
    except LookupError:
        auth_context = {}

    # Instantiate the agent, all fields from the completion_create_params are passed to the agent
    # allowing environment variables to be passed during execution
    agent = MyAgent(**completion_create_params)

    # Invoke the agent and check if it returns a generator or a tuple
    # Set the authorization context in the worker thread before invoking the agent
    def invoke_with_auth_context():  # type: ignore[no-untyped-def]
        try:
            set_authorization_context(auth_context)
        except AttributeError:
            pass

        return event_loop.run_until_complete(
            agent.invoke(completion_create_params=completion_create_params)
        )

    result = thread_pool_executor.submit(invoke_with_auth_context).result()

    # Check if the result is a generator (streaming response)
    if isinstance(result, AsyncGenerator):
        # Streaming response
        return to_custom_model_streaming_response(
            thread_pool_executor,
            event_loop,
            result,
            model=completion_create_params.get("model"),
        )
    else:
        # Non-streaming response
        response_text, pipeline_interactions, usage_metrics = result

        return to_custom_model_chat_response(
            response_text,
            pipeline_interactions,
            usage_metrics,
            model=completion_create_params.get("model"),
        )
