# Copyright 2025 DataRobot, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# mypy: disable-error-code=attr-defined
from typing import Any, Optional, Union

from datarobot_genai.core.agents import extract_user_prompt_content
from datarobot_genai.drmcp.tools.dr_docs import (
    fetch_datarobot_doc_page,
    search_datarobot_agentic_docs,
)
from datarobot_genai.llama_index import (
    DataRobotLiteLLM,
)
from datarobot_genai.llama_index.agent import LlamaIndexAgent
from llama_index.core.agent.workflow import (
    AgentWorkflow,
    FunctionAgent,
)
from llama_index.core.workflow import Context

from agent.config import Config


class MyAgent(LlamaIndexAgent):
    """MyAgent is a DataRobot Agentic AI documentation assistant built on LlamaIndex.
    It utilizes DataRobot's LLM Gateway or a specific deployment for language model interactions.
    This agent implements a two-stage pipeline: a researcher agent that searches DataRobot's
    official agentic AI documentation based on the user's question, and a writer agent that
    synthesizes the findings into a clear, cited response.
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        api_base: Optional[str] = None,
        model: Optional[str] = None,
        verbose: Optional[Union[bool, str]] = True,
        timeout: Optional[int] = 90,
        **kwargs: Any,
    ):
        """Initializes the MyAgent class with API key, base URL, model, and verbosity settings.

        Args:
            api_key: Optional[str]: API key for authentication with DataRobot services.
                Defaults to None, in which case it will use the DATAROBOT_API_TOKEN environment variable.
            api_base: Optional[str]: Base URL for the DataRobot API.
                Defaults to None, in which case it will use the DATAROBOT_ENDPOINT environment variable.
            model: Optional[str]: The LLM model to use.
                Defaults to None.
            verbose: Optional[Union[bool, str]]: Whether to enable verbose logging.
                Accepts boolean or string values ("true"/"false"). Defaults to True.
            timeout: Optional[int]: How long to wait for the agent to respond.
                Defaults to 90 seconds.
            **kwargs: Any: Additional keyword arguments passed to the agent.
                Contains any parameters received in the CompletionCreateParams.

        Returns:
            None
        """
        super().__init__(
            api_key=api_key,
            api_base=api_base,
            model=model,
            verbose=verbose,
            timeout=timeout,
            **kwargs,
        )
        self.config = Config()
        self.default_model = self.config.{{ _external_data.llm.llm_app_name|replace("-", "_") }}_default_model

        if model in ("unknown", "datarobot-deployed-llm"):
            self.model = self.default_model

    def make_input_message(self, completion_create_params: Any) -> str:
        """Create an input string for the workflow.

        Chat history is injected via the `{chat_history}` placeholder.
        The base class (LlamaIndexAgent) will detect this placeholder and
        replace it with the actual history summary when available.
        """
        user_prompt_content = extract_user_prompt_content(completion_create_params)
        # NOTE: The `{{ '{{chat_history}}' }}` syntax below is a Jinja2 template escape pattern.
        # It outputs the literal string `{chat_history}` at render time, which the base class
        # then replaces at runtime with a formatted "Prior conversation:\n..." section when
        # history is available, or with an empty string on the first turn.
        return f"{user_prompt_content}{{ '{{chat_history}}' }}"

    def build_workflow(self) -> AgentWorkflow:
        return AgentWorkflow(
            agents=[self.agent_researcher, self.agent_writer],
            root_agent=self.agent_researcher.name,
            initial_state={
                "researcher_notes": {},
                "report_content": "Not written yet.",
            },
        )

    def extract_response_text(self, result_state: Any, events: list[Any]) -> str:
        """Safely extract the final report text from the workflow state or events."""
        # Preferred path: use the workflow state when available
        if isinstance(result_state, dict) and "report_content" in result_state:
            return str(result_state["report_content"])

        # Fallback: best-effort extraction from the last event that carries text.
        for event in reversed(events or []):
            resp = getattr(event, "response", None)
            if resp is not None and getattr(resp, "content", None):
                return str(resp.content)
            if hasattr(event, "delta") and getattr(event, "delta", None):
                return str(event.delta)
            if hasattr(event, "text") and getattr(event, "text", None):
                return str(event.text)

        # Last resort: empty string
        return ""

    def llm(
        self,
        auto_model_override: bool = True,
    ) -> DataRobotLiteLLM:
        """Returns the DataRobotLiteLLM to use for a given model.

        If a `self.model` is provided, it will be used. Otherwise, the default model will be used.
        If auto_model_override is True, it will try and use the model specified in the request
        but automatically back out to the default model if the LLM Gateway is not configured.

        Args:
            auto_model_override: Optional[bool]: If True, it will try and use the model
                specified in the request but automatically back out if the LLM Gateway is
                not available.

        Returns:
            DataRobotLiteLLM: The model to use.
        """
        api_base = self.litellm_api_base(self.config.{{ _external_data.llm.llm_app_name|replace("-", "_") }}_deployment_id)
        model = self.model or self.default_model
        if auto_model_override and not self.config.use_datarobot_llm_gateway:
            model = self.default_model
        if self.verbose:
            print(f"Using model: {model}")

        config = {
            "model": model,
            "api_base": api_base,
            "api_key": self.api_key,
            "timeout": self.timeout,
        }

        if not self.config.use_datarobot_llm_gateway and self._identity_header:
            config["additional_kwargs"] = {"extra_headers": self._identity_header}  # type: ignore[assignment]

        return DataRobotLiteLLM(**config)

    @property
    def _researcher_tools(self) -> list[Any]:
        """Return tools for the researcher agent.

        Prefers the MCP-served version of search_datarobot_agentic_docs when available
        (i.e. DR_DOCS tools are enabled on the connected MCP server). Falls back to
        calling the imported function directly so the researcher always has access to
        documentation search regardless of MCP configuration.
        """
        if any(t.name == "search_datarobot_agentic_docs" for t in self.mcp_tools):
            return [self.researcher_notes_tool, *self.mcp_tools]
        return [
            search_datarobot_agentic_docs,
            self.researcher_notes_tool,
            *self.mcp_tools,
        ]

    @property
    def _writer_tools(self) -> list[Any]:
        """Return tools for the writer agent.

        Prefers the MCP-served version of fetch_datarobot_doc_page when available.
        Falls back to calling the imported function directly.
        """
        if any(t.name == "fetch_datarobot_doc_page" for t in self.mcp_tools):
            return [self.writer_report_tool, *self.mcp_tools]
        return [fetch_datarobot_doc_page, self.writer_report_tool, *self.mcp_tools]

    @property
    def agent_researcher(self) -> FunctionAgent:
        return FunctionAgent(
            name="ResearcherAgent",
            description="Search DataRobot's Agentic AI documentation to find relevant pages for the user's question.",
            system_prompt=(
                "You are a DataRobot Agentic AI Documentation Researcher.\n"
                "\n"
                "Your responsibilities:\n"
                "1. Analyze the chat history and user question to understand what is being asked. "
                "Consider context from all prior messages. Summarize the user's core question.\n"
                "2. Determine whether the question is related to DataRobot's Agentic AI offerings.\n"
                "3. If the question IS related to DataRobot Agentic AI, use the search_datarobot_agentic_docs "
                "tool to find relevant documentation pages. Make 1 search always. Always use max_results = 3\n"
                "\n"
                "Record notes that follow this strict format:\n"
                "**User question**: {put user question here}\n"
                "**Is the question related to DataRobot Agentic AI?**: {Yes or No}\n"
                "**Relevant documentation**:\n"
                "Title 1: {title of doc page 1}\n"
                "URL 1: {url of doc page 1}\n"
                "Title 2: {title of doc page 2}\n"
                "URL 2: {url of doc page 2}\n"
                "Title 3: {title of doc page 3}\n"
                "URL 3: {url of doc page 3}\n"
                "\n"
                "Once you've recorded your notes, hand off control to the WriterAgent."
            ),
            llm=self.llm(),
            tools=self._researcher_tools,
            can_handoff_to=["WriterAgent"],
        )

    @property
    def agent_writer(self) -> FunctionAgent:
        return FunctionAgent(
            name="WriterAgent",
            description=(
                "Answer the user's question accurately and concisely, "
                "grounded in official DataRobot Agentic AI documentation with citations."
            ),
            system_prompt=(
                "You are a DataRobot Agentic AI expert who answers user questions using official documentation.\n"
                "\n"
                "You receive research from a documentation researcher that includes:\n"
                "- The user's core question about DataRobot Agentic AI\n"
                "- Titles and URLs of relevant DataRobot documentation pages\n"
                "\n"
                "Your responsibilities:\n"
                "1. If the question is not related to DataRobot's Agentic AI offerings, politely acknowledge "
                "this and ask user to input a DataRobot Agentic AI related question.\n"
                "2. Otherwise, use the fetch_datarobot_doc_page tool to retrieve ONLY THE THREE pages that were provided to you.\n"
                "3. Answer the user's question accurately and directly, grounded in the provided documentation.\n"
                "4. Cite specific documentation pages (with URLs) to support your answer.\n"
                "\n"
                "Your output should always follow this strict format:\n"
                "**User question**: {put user question here}\n"
                "**Answer**: {your answer to the user's question here. DO NOT EXCEED ONE OR TWO PARAGRAPHS IN YOUR ANSWER.}\n"
                "\n"
                "Write in markdown format. Once complete, use the writer_report_tool to record your response."
            ),
            llm=self.llm(),
            tools=self._writer_tools,
            # Writer is terminal in this flow; no handoff
        )

    @staticmethod
    async def researcher_notes_tool(ctx: Context, notes: str, notes_title: str) -> str:
        async with ctx.store.edit_state() as current_state:
            if "researcher_notes" not in current_state:
                current_state["researcher_notes"] = {}
            current_state["researcher_notes"][notes_title] = notes
        return "Notes recorded."

    @staticmethod
    async def writer_report_tool(ctx: Context, report_content: str) -> str:
        async with ctx.store.edit_state() as current_state:
            current_state["report_content"] = report_content
        return "Report written."
